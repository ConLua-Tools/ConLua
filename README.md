<div align="center">

# ðŸš€ ConLua: LightRAG & CloudflareAPI based Vietnamese fire safety regulation Retrieval-Augmented Generation
</div>

## Current functionalities

## Deployment
### Environment variables
* LLM_MODEL: This would be the model that does your knowledge graph query and response.
  - Orignal [lightRAG repo](https://github.com/HKUDS/LightRAG/) suggests a model with at least 32b params. Though the example directory used a 8b llama model (`@cf/meta/llama-3.2-3b-instruct` that worked moderately well.
  - Context window of at least 32KB
* EMBEDDING_MODEL:
  - LightRAG is very picky when it comes to embedder, so please use a mainstream one like `@cf/baai/bge-m3`
* CLOUDFLARE_API_KEY: Generate an API key at [https://dash.cloudflare.com](https://dash.cloudflare.com)
* API_BASE_URL: Generate an API base url at [https://dash.cloudflare.com](https://dash.cloudflare.com)
* WORKING_DIR="", for now put `QCVN-06-2022-BXD.docx`. If file upload functionality is fully added, this env variable would become deprecated since the working directory (knoweledge graph and base text storage) is determined by the source document.
* USER_DATA_DIR="" &  JWT_SECRET="" are optional

### Backend deployment
`app.py` runs a FastAPI server with Uvicorn. Just makesure your deployment server has all the requirements from `backend/requirements.txt` (same directory as `app.py`. Example website currently uses Hugging face's spaces

### Frontend deployment
Currently, the front end is only a simple HTML, so feel free to deploy it anyway that you would like. Example website currently uses Vercel

### Quickstart
If you want to visualize how LightRAG works, and specifically how you can use Cloudflare worker API to interact with it, refer to [this link](https://github.com/HKUDS/LightRAG/blob/main/examples/unofficial-sample/lightrag_cloudflare_demo.py)

## Directory overview
This repo is supposed to be a building block for further projects, so it's kept relatively simple.
### \lib
* backRetrieval.py: This is the barebone for direct citation from source based on responded info from the chat bot.
* cloudflareWorker.py: This contains a class that allows communication with cloudflareworker api that allows for both querying and embedding operations.
* lightrag\_extensions.py: This contains a class inherited from the base lightrag class from the lightrag library that specifically uses cloudflare worker API.
* pydantic\_filters.py: Simply filters for input fields.
* SimpleKnowledgeStore.py: This file allows for chunk-based Retrieval-Augmented Generation. This is what is currently being used instead of true LightRAG due to cost concerns.
### \lightrag
This is a modified version of the LightRAG library that is lighter without unecessary directories. `prompt.py` and `operate.py` were modified to accomodate for LLM without context caching and extra examples specific to Vietnamese fire safety regulations.
### \QCVN-06-2022-BXD.docx
Example working directory for lightRAG. It contains:
* The original text
* Knowledge graph/chunk data generated by lightRAG from the original text
### \app.py
FastAPI server running with Uvicorn. Though purely a web API, endpoints perform functions that can be directly used in other files for processing data and respoding.

## Planned development
Possibe development paths for ConLua.
### True lightRAG functionality
#### Retrieval
Add back lightRAG querying modes (please refer to [this paper](https://arxiv.org/abs/2410.05779)) (or knowledge-graph-based augmented retrieval) without dramaticaly increasing cost. Can be achieved by tinkering with data retrieval and reranking model. Another possible avenue is to not rely on CloudflareAPI and run our local LLM, which allows for LLM caching.
#### Reranker model
Add a reranker model, which according to the original paper, would drastically improve lightRAG's performance
### Multiple working directory shared through server backend
Allows for file uploads that:
* Create a new working directory (via os)
* Creates new knowledge graph (via `lightrag_extensions.py`)
And that directory is shared across users to avoid duplications and unecessary LLM queries towards knowledge graph creation.
### LLM cache
LLM caching allows for more efficient retrieval, especially queries that requires source citations.

### Functionalities
* Data visualization. LightRAG can generate 4 different type of graph knowledge. ConLua currently outputs a graphml file, which perhaps is not the best in data visualization. It would be nice if regulations can be discovered as more beautiful graphs.
* CAD & BIM software integration. Possible method: repackage website using electronJS and ship it within the software's addon templates.
* CAD & BIM software integrated compliance checking. Program receives a design file from software, run it through checks generated by a different program that processed regulatory documents, then return passed and failed checks.
* CAD & BIM software context-based RAG. In addition with LighRAG, program can allow users to select design elements, highlight which relationship between them, then generate a query towards ligthRAG using such information
